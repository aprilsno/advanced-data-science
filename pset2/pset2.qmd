---
format: html
figure:
  align: center
---

# Problem Set 2 {.unnumbered}

## Introduction {.unnumbered}

For this assignment, you'll delve into data wrangling, statistical inference, and linear modeling that was used by academics to gain a deeper understanding of the efforts made to estimate the indirect death toll in Puerto Rico following Hurricane María. Begin by reviewing [this comprehensive timeline and summary](https://simplystatistics.org/posts/2018-09-28-the-complex-process-of-obtaining-puerto-rico-mortality-data-a-timeline/). Initially, we'll use data wrangling techniques to extract information from documents released by organizations that had early access to the mortality registry data. Following that, we'll work with the mortality registry data that has since been publicly disclosed by the government. To determine mortality rates, it's essential to acquire data on population size, categorized by age and sex. We'll achieve this by utilizing APIs provided by the US Census.

These are the libraries you will need and the only ones you are allowed to load
```{r}
#| warning: false
#| message: false
library(readr)
library(dplyr)
library(forcats)
library(lubridate)
library(tidyr)
library(stringr)
library(pdftools)
library(janitor)
library(httr2)
library(excessmort)
library(jsonlite)
library(purrr)
```

You don't need these but we will allow you to load them:
```{r}
#| warning: false
#| message: false
library(ggthemes)
library(ggrepel)
```

Reminders:

* Add a title to all your graphs.
* Add a label to the x and y axes when not obvious what they are showing.
* Think about transformations that convey the message in clearer fashion.

## Preparation {.unnumbered}

Create a directory for this homework. In this directory create two subdirectories: `data` and `rdas`. You will also create a `get-population.R` file where you will have the code to download and wrangle population data from the US Census.

## Wrangling {.unnumbered}

(@) In December 2017 a preprint was published that includes data from the mortality registry. It is a Word document that you can download from <https://osf.io/preprints/socarxiv/s7dmu/download>. Save a PDF copy of this document to your data directory.


(@) Read in the PFD file into R and create a data frame with the data in Table 1 of the paper. The data frame should be tidy with columns `months`, `year`, and `deaths`. Your data frame need not include the confidence intervals or averages.
```{r}
fn <- '~/pset2/data/santoslozada-howard-2017-preprint.pdf'
txt <- pdf_text(fn)[4]
tmp <- str_split(txt, "\n")[[1]][2:14] %>% 
  str_replace_all("\\s([A-Z])", "\\1") %>% 
  str_replace_all("\\s-\\s", "-") %>% 
  str_split("\\s+", simplify = TRUE)
tmp[1,1] <- "month"

q2 <- tmp %>% 
  row_to_names(1) %>% 
  as.data.frame() %>% 
  select(c(month, `2010`:`2016`)) %>% 
  pivot_longer(-month, names_to = 'year', values_to = 'deaths') %>% 
  mutate(month = match(month, month.name),
         year = as.numeric(year),
         deaths = parse_number(deaths))
```

(@) For each month compute the average and a 95% confidence interval to reproduce Figure 3 in the preprint.
Make sure to show the month names on the x-axis, not numbers. Hint: Save the graph to an object to make an upcoming exercise easier.
```{r, fig.height=5, fig.width=9}
q2 %>% 
  as.data.frame() %>% 
  group_by(month) %>% 
  mutate(avg = sum(deaths)/n(),
         se = sd(deaths)/sqrt(n()),
         lower_ci = avg - qt(0.975, df = n() - 1) * (se),
         upper_ci = avg + qt(0.975, df = n() - 1) * (se)) %>% 
  mutate(month = factor(month, levels = 1:12, labels = month.name)) %>% 
  ggplot(aes(x = month, y = avg)) +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci, color = "95% C.I."), width = 0.2) +
  geom_point(aes(color = 'Mean'), size = 2) + 
  labs(title = "Figure 3: Average deaths and 95% confidence intervals by month in Puerto Rico, 2010-2016",
       x = "Month",
       y = "Mean and 95% C.I.") +
  scale_y_continuous(breaks = seq(2200, 2900, by = 100), limits = c(2200, 2900)) +
  scale_color_manual(values = c("Mean" = "red", "95% C.I." = "black")) +
  guides(color = guide_legend(nrow = 1, byrow = TRUE, 
                              override.aes = list(shape = c(NA, 16), linetype = c("solid", "blank")))) +
  theme(axis.ticks = element_blank(), legend.position = "bottom", legend.title = element_blank())
```

(@) The model here seems to be that the observed death for month $i$ and year $j$ is 

$$
Y_{ij} = \mu_i + \varepsilon_{ij}
$$
with $\text{Var}(\varepsilon_{ij}) = \sigma^2_i$. The preprint reports the September and October 2017 deaths as 2,987 and 3,043. Create a data frame called `dat_2017` with these two values and include an estimate for the standard error of this random variable. Hint: Look at the model and use data from 2010-2016 to estimate $\sigma_i$.
```{r}
tmp <- q2 %>%
  group_by(month) %>%
  summarize(mu = mean(deaths),
            sigma = sd(deaths),
            var = sigma^2,
            se = sigma/sqrt(n()),
            lower_ci = mu - qt(0.975, df = n() - 1) * (se),
            upper_ci = mu + qt(0.975, df = n() - 1) * (se)) 

dat_2017 <- data.frame(
  month = c(9,10),
  mu = c(2987, 3043)) %>% 
  left_join(tmp %>% select(-mu), by = 'month') %>%
  mutate(lower_ci = mu - 1.96*sigma,
         upper_ci = mu + 1.96*sigma,
         month = factor(month, levels = 1:12, labels = month.abb))
  
tmp <- tmp %>% mutate(month = factor(month, levels = 1:12, labels = month.abb))
```

(@) Make a plot now that includes the two points for 2017 and the 1.96 standard errors bars around them. Are the deaths statistically significantly different than the expected based on 2010-2016 data? 
```{r}
ggplot() +
  geom_errorbar(data = tmp, aes(x = month, ymin = lower_ci, ymax = upper_ci), color = 'red', width = 0.2) +
  geom_point(data = tmp, aes(x = month, y = mu), color = 'black', size = 1) +
  geom_errorbar(data = dat_2017, aes(x = month, ymin = lower_ci, ymax = upper_ci), color = 'blue', width = 0.2) +
  geom_point(data = dat_2017, aes(x = month, y = mu), color = 'blue', size = 1) +
  labs(title = "Average deaths and 95% CIs by month in PR, 2010-2016",
       subtitle = "Observed deaths for Sep and Oct, 2017 in blue",
       x = "Month",
       y = "Avg deaths and 95% C.I. of Deaths")
```
Yes, the deaths are statistically different from the expected based on 2010-2016 data. We know this to be true because the observed number of deaths in September and October of 2017 fall outside the constructed confidence intervals for the expected number of deaths.

(@) On December 8, 2017 the New York Times publishes an article with daily counts. They share the data that was provided to them by the Mortality Registry. It is PDF you can obtain [here](https://github.com/c2-d2/pr_mort_official/raw/master/data/Mortalidad-RegDem-2015-17-NYT-part1.pdf). 
Read the PDF into R and extract the daily counts. Save the results to a data frame called `dat` with columns `data` and `deaths`. Make sure the data frame is ordered by date. 
```{r}
url <- "https://github.com/c2-d2/pr_mort_official/raw/master/data/Mortalidad-RegDem-2015-17-NYT-part1.pdf"
pdf <- pdf_text(url) |> str_split("\n")
dat <- lapply(pdf, function(s){
  s <- str_trim(s)
  s <- str_remove_all(s, "Registro Demográfico - División de Calidad y Estadísticas Vitales")
  header_index <- str_which(s, "2015")[1]
  tmp <- str_split(s[header_index], "\\s+", simplify = TRUE) |> str_remove_all("\\*") |>
    str_replace_all("Y(201\\d)", "\\1")
  month <- tmp[1]
  header <- tmp[-c(1,5)]
  tail_index  <- str_which(s, "Total")
  n <- str_count(s, "\\d+")
  out <- c(1:header_index, ## take out first lines
           which(n <= 3), ## lines with just one number (plot y-axis ) or 3 (legend)
           which(n >= 20 & n <= 31), ## take out lines with just numbers from plot x-axis
           tail_index:length(s)) ## take out lines at end
  if (month == "FEB") {
   feb29 <- s[str_detect(s, "^29\\s+")] |> str_remove("29\\s+") |> parse_number()
  }
  s <- s[-out] |>  
    str_remove_all("[^\\d\\s]") |> ## remove things that are not digits or space
    str_trim() |> 
    str_split_fixed("\\s+", n = 6)  ## split by any space
  
  if (month == "DEC") {
    header <- header[1:2]
    s <- s[,1:3]
  } else {
    s <- s[,1:4]
  }
  colnames(s) <- c("day", header)
  
  s <- s |> as_tibble() |> 
    mutate(month = month, day = as.numeric(day)) |>
    pivot_longer(-c(day, month), names_to = "year", values_to = "deaths") |>
    mutate(deaths = as.numeric(deaths), month = str_to_title(month)) |>
    mutate(month = if_else(month == "Ago", "Aug", month)) |>
    mutate(month = match(month, month.abb)) |>
    mutate(date = make_date(year, month, day)) |>
    select(date, deaths) |>
    arrange(date)

  if (month == "FEB") {
    s <- bind_rows(s, data.frame(date = make_date(2016, 2, 29), deaths = feb29)) 
  }
 
   return(s)
})
dat <- do.call("bind_rows", dat) |> arrange(date)
```

(@) Plot the deaths versus dates and describe what you see towards the end for 2017.
```{r}
ggplot(dat, aes(x = date, y = deaths)) +
  geom_point() +
  labs(title = "Daily Death Counts, 2015-2017", x = "Date", y = "Number of Deaths")
```
Towards the end for 2017, the plot shows a marked decline in the number of deaths.

(@) The reason you see a drop at the end is because it takes time to officially register deaths. It takes about 45 days for 99% of the data to be added. Remove the last 45 days and remake the plot, but this time showing deaths against day of the year (1 through 365 or 366) with color highlighting what happened after the hurricane. Do not include a legend.
```{r}
dat$day_of_year <- yday(dat$date)
dat <- head(dat, n = nrow(dat) - 45)
dat <- dat %>% 
  mutate(after_maria = case_when(
    date < '2017-09-20' ~ 0,
    TRUE ~ 1),
    year = year(date))

ggplot(dat, aes(x = day_of_year, y = deaths, color = factor(after_maria))) +
  geom_point() +
  labs(title = "Daily Death Counts, 2015-2017",
       subtitle = "Deaths after Hurricane María in 2017 are in red",
       x = "Day of the Year", 
       y = "Number of Deaths") +
  theme(legend.position = "none") +
  scale_color_manual(values = c('black','red')) 
```
We can see that Hurricane María caused a peak that was above average for that time of year compared to 2015 and 2016.

## US Census APIs

In June 2018, data was finally made public. This dataset gives you deaths by age group and sex obtained more recently from the Mortality Registry. In preparation for the analysis of these data, we will obtain population estimates from the US Census by age and gender. 

We will be using two different APIs as that is how the Census makes the data available. Important to note that in two of these APIs, all ages 85 or above are grouped into one group. 

If you wish to skip this section (though you will lose points), you can obtain the already wrangled population data [here](https://github.com/datasciencelabs/2023/raw/main/data/population.rds).

(@) First step is to obtain a census key. You can request one here <https://api.census.gov/data/key_signup.html>.  Once you have a key create a file in your directory called `census-key.R` that simply defines the variable `census_key` to be your personal key. Do not share this key publicly. The quarto file you turn in should not show your census key, instead it should source a file called `census-key.R` to define the variable. We will have a file on our end with our key so your script can knit.

(@) Once you have your key you can use the `httr2` package to download the data directly from the Census data base. We will start downloading the intercensus data from 2000-2009 ([data dictionary here](https://www.census.gov/data/developers/data-sets/popest-popproj/popest/popest-vars.2000-2010_Intercensals.html#list-tab-794389051)). We will download it only for Puerto Rico which has region ID 72. The following code downloads the data.

```{r}
#| eval: true
url <- "https://api.census.gov/data/2000/pep"
source("census-key.R")
endpoint <- paste0("int_charage?get=POP,SEX,AGE,DATE_&for=state:72&key=", census_key)
response <- request(url) |> 
  req_url_path_append(endpoint) |>
  req_perform()  
```

The data is now included in `response` and you can access it using the `resp` functions in **httr2**. Examine the results you obtain when applying `resp_body_string`. Write code to convert this into a data frame with columns names `year`, `sex`, `age`, and `population` and call it `pop1`. Hint: Use the function `fromJSON` from the **jsonlite** package. The functions `row_to_names` and `clean_names` from the **janitor** package might also be handy. Use the codebook to understand how the `date` column relates to year.
```{r}
pop1 <- response %>%
  resp_body_string() %>%
  fromJSON(flatten = TRUE) %>%
  as.data.frame() %>%
  row_to_names(1) %>%
  clean_names() %>%
  mutate(across(everything(), parse_number)) %>% # turn all character variables into numeric
  filter(age != 999 & sex != 0) %>% # drop values not of interest
  mutate(sex = factor(sex, labels = c("M", "F")),
         year = 2000) %>%
  filter(between(date, 2, 11)) %>%
  mutate(year = year + date - 2) %>%
  rename(population = pop) %>%
  select(c(year, sex, age, population))
```


(@) Now we will obtain data for 2010-2019. The intercensal data is not available so we will use _Vintage_ 2019 data ([data dictionary here](https://www.census.gov/data/developers/data-sets/popest-popproj/popest/popest-vars.Vintage_2019.html)). We can follow a similar procedure but with the following API and endpoints:

```{r}
#| eval: true
url <- "https://api.census.gov/data/2019/pep"
source("census-key.R")
endpoint <- paste0("charage?get=POP,SEX,AGE,DATE_CODE&for=state:72&key=", census_key)

response2 <- request(url) |> 
  req_url_path_append(endpoint) |>
  req_perform()  
```

Download the data and write code to convert this into a data frame with columns names `year`, `sex`, `age`, and `population` and call it `pop2`. 
```{r}
pop2 <- response2 %>%
  resp_body_string() %>%
  fromJSON(flatten = TRUE) %>%
  as.data.frame() %>%
  row_to_names(1) %>%
  clean_names() %>%
  mutate(across(everything(), parse_number)) %>% # turn all character variables into numeric
  filter(age != 999 & sex != 0) %>% # drop values not of interest
  mutate(sex = factor(sex, labels = c("M", "F")),
         year = 2010) %>%
  filter(between(date_code, 3, 12)) %>%
  mutate(year = year + date_code - 3) %>%
  rename(population = pop) %>%
  select(c(year, sex, age, population))
```

(@) Combine the data frames `pop1` and `pop2`  created in the previous exercises to form one population 
data frame called `population` and including all year. Make sure the 85+ category is correctly computed on the two datasets.
Save it to a file called `population.rds` in your rds. 
```{r}
pop2 <- pop2 %>% 
  mutate(age = pmin(age, 85)) %>% 
  group_by(sex, age, year) %>% 
  summarize(population = sum(population), .groups = 'drop')

population <- bind_rows(pop1, pop2) 
  
saveRDS(population, "./rdas/population.rds") 
```

## Daily count data {.unnumbered}

Let's repeat the analysis done in the preprint but now using 2002-2016 data and, to better see the effect of the hurricane, let's use weekly instead of monthly and start our weeks on the day the hurricane hit.

You can load the data from the **excessmort** package.

```{r}
data("puerto_rico_counts")
```

(@) Define an object `counts` by wrangling `puerto_rico_counts` to 1) include data only from 2002-2017, 2) remove the population column, and 3)  to match our population, combine the counts for those 85 and older together.
```{r}
counts <- puerto_rico_counts %>% 
  # filter(between(year(date), 2002, 2017)) %>% 
  filter(year(date) >= 2002 & year(date) <= 2017) %>% 
  select(-population) %>% 
  # mutate(agegroup = fct_collapse(agegroup, "85+" = c('85-89','90-94','95-99','100-Inf'))) %>%
  mutate(agegroup = ifelse(agegroup %in% c('85-89','90-94','95-99','100-Inf'), "85+", as.character(agegroup))) %>%  # Combine counts for those 85 and older
  group_by(date, sex, agegroup) %>%
  summarize(outcome = sum(outcome), .groups = 'drop') 
```

(@) Collapse the population data so that it combines agegroups like `counts`. Also change the `sex` column so that it matches `counts` as well.
```{r}
cuts <- c(seq(0,85,5), Inf)
labels <- paste0(seq(0, 80, 5), "-", seq(4, 84, 5))
labels <- c(labels, "85+")

population <- population %>% 
  mutate(
    sex = ifelse(sex == 'M', 'male', 'female'),
    agegroup = cut(age, breaks = cuts, labels = labels, right = FALSE)
  ) %>% 
  group_by(year, sex, agegroup) %>% 
  summarize(population = sum(population), .groups = 'drop')

# Alternate approach
# population <- population %>% 
#   mutate(sex = ifelse(sex == 'M', 'male', 'female'),
#          agegroup = case_when(
#            age >= 0 & age <= 4 ~ '0-4',
#            age >= 5 & age <= 9 ~ '5-9',
#            age >= 10 & age <= 14 ~ '10-14',
#            age >= 15 & age <= 19 ~ '15-19',
#            age >= 20 & age <= 24 ~ '20-24',
#            age >= 25 & age <= 29 ~ '25-29',
#            age >= 30 & age <= 34 ~ '30-34',
#            age >= 35 & age <= 39 ~ '35-39',
#            age >= 40 & age <= 44 ~ '40-44',
#            age >= 45 & age <= 49 ~ '45-49',
#            age >= 50 & age <= 54 ~ '50-54',
#            age >= 55 & age <= 59 ~ '55-59',
#            age >= 60 & age <= 64 ~ '60-64',
#            age >= 65 & age <= 69 ~ '65-69',
#            age >= 70 & age <= 74 ~ '70-74',
#            age >= 75 & age <= 79 ~ '75-79',
#            age >= 80 & age <= 84 ~ '80-84',
#            age >= 85 ~ '85+')) %>% 
#   select(-age) %>% 
#   group_by(year, sex, agegroup) %>% 
#   summarize(population = sum(population), .groups = 'drop')
```

(@) Add a population column to `counts` using the `population` data frame you just created.
```{r}
agelevels <- levels(population$agegroup)
counts <- counts %>% 
  mutate(year = year(date)) %>% 
  left_join(population, by = c('year', 'sex', 'agegroup')) %>% 
  select(-year) %>%
  mutate(agegroup = factor(agegroup, levels = agelevels))
```

(@) Use R to determine what day of the week did María make landfall in PR.
```{r}
wday(as.Date('2017-09-20'))
weekdays(as.Date('2017-09-20'))
```

(@) Redefine the date column to be the start of the week that day is part of. Use the day of the week María made landfall as the first day. Now collapse the data frame to weekly data by redefining `outcome` to have the total deaths that week for each sex and agegroup. Remove weeks that have less the 7 days. Finally, add a column with the MMWR week. Name the resulting data frame `weekly_counts`
```{r}
weekly_counts <- counts %>% 
  mutate(date = floor_date(date, unit = 'week', week_start = 3)) %>%
  group_by(sex, agegroup, date) %>%
  filter(n() >= 7) %>% 
  summarize(outcome = sum(outcome), population = first(population), .groups = 'drop') %>% 
  mutate(mmwr_week = epiweek(date))
```

(@) Make a per-week version of the plot we made for monthly totals. Make a boxplot for each week based on the 2002-2016 data, then add red points for 2017. Comment on the possibility that indirect effect went past October.
```{r}
weekly_counts_sum <- weekly_counts %>% 
  group_by(date, mmwr_week) %>% 
  summarize(outcome = sum(outcome), .groups = 'drop') 

weekly_counts_sum %>% filter(date <= as.Date('2016-12-31')) %>% 
  ggplot(aes(x = mmwr_week, y = outcome, group = mmwr_week)) +
  geom_boxplot() +
  geom_point(color = "red", data = weekly_counts_sum %>% filter(date >= as.Date('2017-01-01'))) +
  labs(title = 'Deaths per week in PR, 2002-2017', 
       subtitle = '2017 data in red',
       x = 'US CDC epidemiological week',
       y = 'Deaths')
```
Comparing the deaths in 2017 to the trends from 2002-2016, we see that the indirect effects of Hurricane María on the deaths per week did continue past October, as 2017 data is falling above the median in the months following the hurricane.

(@) If we look at 2017 data before September and compare each week to the average from 2002-2016. What percent are below the median?
```{r}
weekly_counts_sum %>%
  filter(year(date) < 2017) %>%
  group_by(mmwr_week) %>%
  summarize(median = median(outcome)) %>%
  left_join(filter(weekly_counts_sum, year(date) == 2017), by='mmwr_week') %>%
  filter(date < as.Date('2017-09-01')) %>%
  ungroup() %>%
  summarize(percent_below = mean(outcome < median)*100)

# Alternate approach
# weekly_counts_sum %>%
#   group_by(mmwr_week) %>%
#   mutate(median = median(outcome[year(date) != 2017])) %>% 
#   filter(year(date) == 2017 & date < as.Date('2017-09-01')) %>% 
#   ungroup() %>% 
#   summarize(percent_below = mean(outcome < median)*100)
```

(@) Why are 2017 totals somewhat below-average? Plot the population in millions against date. What do you see?
```{r}
counts %>% 
  group_by(date) %>% 
  summarize(total_population = sum(population)) %>% 
  ggplot(aes(x=date, y=total_population/1000000)) +
  geom_smooth(method = 'gam', color="blue") +
  labs(title = "Population of PR, 2002-2017",
       x = "Date",
       y = "Population (millions)")
```
The population is decreasing over time. This would lead to weekly averages below what we would expect in later years.

(@) When comparing mortality across populations of different sizes, we need to look at rates not totals. 
Because the population is decreasing, this is particularly important. Redo the boxplots but for rates instead of totals.
```{r}
tmp <- weekly_counts %>% 
  group_by(date) %>% 
  mutate(rate = sum(outcome)/sum(population) * 100000) 

tmp %>% filter(date <= as.Date('2016-12-31')) %>% 
  ggplot(aes(x = mmwr_week, y = rate, group = mmwr_week)) +
  geom_boxplot() +
  geom_point(color = "red", data = tmp %>% filter(date >= as.Date('2017-01-01'))) +
  labs(title = 'Death rates per week, 2002-2017', 
       subtitle = '2017 data in red',
       x = 'US CDC epidemiological week',
       y = 'Death rate/100,000')
```

(@) Now the rates are all way above average! What is going on? Compute and plot the population sizes against year for each sex of the following age groups: 0-19, 20-39, 40-59, 60+. Describe what you see in this plot then explain why 2017 has higher average death rates.
```{r}
counts %>% 
  mutate(agegroup = fct_collapse(
    agegroup,
    "0-19" = c("0-4", "5-9", "10-14", "15-19"),
    "20-39" = c("20-24", "25-29", "30-34", "35-39"),
    "40-59" = c("40-44", "45-49", "50-54", "55-59"),
    "60+" = c("60-64", "65-69", "70-74", "75-79", "80-84", "85+")),
    year = year(date)) %>% 
  group_by(year, date, sex, agegroup) %>% 
  summarize(population = sum(population), .groups = 'drop') %>% 
  ggplot() +
  geom_line(aes(x = year, y = population/10^5, color = sex)) +
  facet_wrap(~agegroup, ncol = 2) + 
  labs(title = 'Population size over time by age group',
       x = 'Year',
       y = 'Population/100,000')
```
For both males and females, the population across all age groups other than 60+ decreases over time and the population for 60+ individuals increases over time. 2017 has a higher death rate because the denominator, or population, is smaller and the resulting rate is larger.

(@) Compute the death rates (deaths per 1,000 per year) by the agegroups for each year 2002-2016. Use a transformation of the y-axis that permits us to see the data clearly. Make a separate plot for males and females. Describe in two sentences what you learn.
```{r}
counts %>%
  mutate(year = year(date)) %>%
  filter(year < 2017) %>%
  group_by(year, sex, agegroup) %>%
  summarize(rate = sum(outcome) / first(population) * 1000, .groups = 'drop') %>% # rate = mean(outcome)/population[1]*1000*52
  ggplot() +
  geom_line(aes(x = year, y = rate, color = agegroup)) +
  facet_wrap(~sex, ncol=2) + 
  labs(title = 'Death rates over time by age group',
       x = 'Year',
       y = 'Death rate per 1,000',
       color = 'Age group') +
  scale_y_continuous(trans = "log10") 
```
We see that for both males and females, death rates typically increase as age group increases and rates have been somewhat constant or decreasing over time, with an exception of a drastic decrease in the rate among the a few of the younger age groups in 2016.

(@) Repeat the above but use `facet_wrap` with `scales = "free_y"` to get a closer look at the patterns for each age group. In this case use color to distinguish the sexes. Describe the pattern observed for the death rate over time.
```{r}
counts %>% 
  mutate(year = year(date)) %>% 
  filter(year < 2017) %>% 
  group_by(year, sex, agegroup) %>% 
  summarize(rate = sum(outcome)/first(population)*1000, .groups = 'drop') %>% # rate = mean(outcome)/population[1]*1000*52
  ggplot() +
  geom_line(aes(x = year, y = rate, color = sex)) +
  facet_wrap(~agegroup, ncol=5, scales = "free_y") + 
  labs(title = 'Death rates over time by age group',
       x = 'Year',
       y = 'Death rate per 1,000',
       color = 'Sex') +
  scale_x_continuous(breaks = c(2004, 2008, 2012, 2016)) +
  scale_y_log10() +
  theme(axis.text.x = element_text(size = 7))
```
Across the 0-14 age groups, there is a dramatic drop in death rate in 2015 for both males and females. Across other age groups, death rates have decreased over time overall.

## Linear models {.unnumbered}

(@) We are going fit a linear model to account for the trend in death rates to obtain an more appropriate expected death rate for each agegroup and sex. Because we are fitting a linear model, it is preferable to have normally distributed data. We want the number of deaths per week to be larger than 10 for each group.
Compute the average number of deaths per week by agegroup and sex for 2016. Based on these data, what agegroups do you recommend we combine?
```{r}
counts %>% 
  filter(year(date) == 2016) %>% 
  group_by(agegroup, sex) %>% 
  summarize(avgdeaths = sum(outcome)/52, .groups = 'drop') %>% 
  pivot_wider(names_from = sex, values_from = avgdeaths) 
```
The lower age groups should be combined to have at least 10 observations per grouping. Grouping together 0-44 and 45-54 should ensure there are enough deaths per week in each group, based on the average deaths per group calculated above.

(@) Create a new dataset called `dat` that collapses the counts into agegroups with enough deaths to fit a linear model. Remove any week with MMWR week 53 and add a column `t` that includes the number of weeks since the first week in the first year.
```{r}
dat <- weekly_counts %>% 
  filter(mmwr_week != 53) %>% 
  mutate(agegroup = fct_collapse(
    agegroup, "0-44" = c("0-4", "5-9", "10-14", "15-19", "20-24", "25-29", "30-34", "35-39", "40-44"),
              "45-54" = c("45-49", "50-54"))) %>% 
  group_by(date, sex, agegroup, mmwr_week) %>% 
  summarize(outcome = sum(outcome), population = sum(population), .groups = 'drop') %>%  
  mutate(t = (year(date) - 2002) * 52 + mmwr_week - 1) %>% 
  group_by(date, sex, agegroup, mmwr_week) %>% 
  mutate(rate = sum(outcome)/sum(population)) %>% 
  ungroup() 
```


(@) Write a function that receives a data frame `tab`, fits a linear model with a line for the time trend, and returns a data frame with 2017 data including a prediction.
```{r}
fit = function(dat) {
  mod = lm(rate ~ t + as.factor(mmwr_week), data = dat %>% filter(date < as.Date('2017-01-01')))
  pred_df = data.frame(
    dat %>% filter(date > as.Date('2016-12-31')) %>% select(mmwr_week, rate, t, population) 
  )
  pred_df$exp = predict(mod, pred_df)
  pred_df$sd <- summary(mod)$sigma
  
  return(pred_df)
}
```
 
(@) Use the `group_modify` function to fit this model to each sex and agegroup. Save the results in `res`.
```{r}
res <- dat %>% 
  group_by(sex, agegroup) %>% 
  group_modify(~fit(.x)) %>% 
  ungroup
```

(@) For agegroup and by sex, plot the expected counts for each week with an error bar showing two standard deviations and in red the observed counts. Does the model appear to fit? Hint: Look to see if the red dots are inside the intervals before the hurricane.
```{r}
res %>% 
  ggplot(aes(mmwr_week, exp*population)) +
  geom_errorbar(aes(ymin = (exp - 1.96*sd)*population, ymax = (exp + 1.96*sd)*population)) + 
  geom_point() + 
  geom_point(aes(mmwr_week, rate*population), color = "red") +
  facet_grid(agegroup~sex, scales = "free_y") +
  labs(title = '')
```
Yes, the model seemed to fit and the observed counts largely fall within the confidence intervals of the expected values from the model before the hurricane.

(@) Now estimate weekly excess deaths for 2017 based on the rates esimated from 2002-2016 but the population sizes of 2017. Compare this to estimated standard deviation observed from year to year once we account for trends.
```{r}
excess <- res %>% 
  group_by(mmwr_week) %>% 
  summarize(obs = sum(rate*population), 
            exp = sum(exp*population), 
            sd = sqrt(sum(population^2 * sd^2)), .groups = 'drop') %>% 
  mutate(diff = obs - exp) # excess deaths

sd <- unique(excess$sd) 

ggplot(excess) + 
  geom_point(aes(x = mmwr_week, y = diff)) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_hline(yintercept = c(-2,2)*sd, lty = 2, color = 'red') +
  labs(title = 'Estimated weekly excess deaths in 2017',
       x = 'US CDC epidemiological week',
       y = 'Excess deaths')
```

(@) Plot cumulative excess death for 2017 including a standard error.
```{r}
excess %>% 
  mutate(diff = cumsum(diff),
         sd = sqrt(cumsum(sd^2))) %>% 
  ggplot(aes(x = mmwr_week, y = diff)) +
  geom_ribbon(aes(ymin = diff - 2*sd, ymax = diff + 2*sd), alpha = 0.5) + 
  geom_line() + 
  geom_hline(yintercept = 0, lty = 2) +
  labs(title = 'Cumulative excess deaths in 2017',
       x = 'US CDC epidemiological week',
       y = 'Cumulative excess deaths')
```



